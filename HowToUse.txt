Date : 2020/08/14
Author @rotildeof

(未来の自分は他人なので、logを残す。)

目次
1. 概要
2. 使い方
3. TestNN.cxxについて
4. 所感


1. 概要
　ニューラルネットワークの機能をC++で簡易的に実装しました。 このクラス"NeuralNetwork"では、全結合型ニューラルネットワーク(NN)の構造を任意に指定することができます。
 出来るだけ汎用的に設計ができるようにパラメータや使用する活性化関数、学習法を適宜選択できるようにしました。
 具体的に活性化関数に関しては ReLU, Sigmoid, Identity, Softmax(出力層のみ)を備えており、また学習法に関しては、 確率論的勾配法(SDG)、Momentum SDG、Adam、遺伝的アルゴリズム(GA)のオプションがあります。
 さらに、学習したNNを保存し、それを再び活用することも可能です。これらの機能はソースファイル、"GeneticAlgorithm.h", "GeneticAlgorithm.cxx", "NeuralNetwork.h", "NeuralNetwork.cxx" が揃っていれば使用できます。
 短期間で作ったのでところどころ荒い部分やバグもあると思いますが、何か見つけ次第@rotildeofまで連絡をくださると幸いです。




2. 使い方
　クラス"NeuralNetwork"のpublicメンバ関数の使い方の説明です。

NeuralNetwork(std::string structure_)
　コンストラクタです。引数にはNNの構造を指定します。指定の方法ですが、例えば"2:4:3:2"や"1:10:10:3"などです。
　これらの書き方は、入力層から順に出力層までのニューロンの数をコロンで区切った形になっています。
　最初の例で行くと、入力層2個、隠れ層(2層)に4つと3つ、出力層に2つのニューロンがある構造になります。これらの構造は自由に設定可能です。


~NeuralNetwork()
　デストラクタです。特に何もしません。


void SetActivationFunction_Hidden(std::string actFuncHidden)
　隠れ層で用いる活性化関数を指定します。指定できる関数は"ReLU", "Identity", "Sigmoid" の3つです。


void SetActivationFunction_Output(std::string actFuncOutput)
　出力層で使う活性化関数を指定します。指定できる関数は"Identity", "Sigmoid", "Softmax" の3つです。Softmaxは関数の定義上出力層のニューロンが2つ以上の場合にのみ使用できます。


void SetLossFunction(std::string nameLossFunction_)
　損失関数を指定します。使用できる関数は平均二乗誤差("MSE"), 二値クロスエントロピー("BCE")、多値クロスエントロピー("CCE")の3つです。
　なお、出力層の活性化関数で"Identity"を選択した場合、二値および多値クロスエントロピーを使用することはできません。これは、クロスエントロピーを使う場合は出力層の出力が0~1に収まっていなければならないからです。
　また、クロスエントロピーを指定した時は、正解用のデータは 0 か 1 のみで構成するようにしてください。



void SetLearningMethod(std::string method)
　NN の学習法を指定します。指定できる学習法は確率論的勾配降下法("SGD"), "MomentumSGD", "Adam" の3つです。


void TrainNN(vec2D const &inputDataSet,
             vec2D const &answerDataSet,
	           int nRepetitions)
　NNを学習させます。第１引数に学習用入力データセット、第２引数に正解データセット、第３引数にイタレーションの回数(学習の回数)を指定します。
 入力および正解データセットのデータ構造は、二次元のvector配列で表現します。vec[i][j]があったとき、[i] がデータエントリーに対応し、[j] が入力ニューロンに対応します。[i]のサイズがデータ数、[j]のサイズが入力/出力ニューロンの数に一致します。もちろん、入力と出力データは事前に1対1に対応させる必要があります。


void LearningGA(vec2D const &inputDataSet,
     		        vec2D const &answerDataSet,
		            int nRepetitions)
　NNを遺伝的アルゴリズムによって学習させます。この手法は非常に低速なため、使用は非推奨です。第3引数にイタレーションの回数を指定します。


void LearningGA(vec2D const &inputDataSet,
		            vec2D const &answerDataSet,
		            double threshold)
　NNを遺伝的アルゴリズムによって学習させ、Loss関数の出力値が threshold 以下になったときに学習をやめます。
　原因はわかってませんが、イタレーションを指定した場合に比べ、学習の収束速度が遅くなる傾向にあります。いずれにせよ、遺伝的アルゴリズムによる学習は非推奨です。


void SetLossPrintFrequency(int freq)
　学習時、どれくらいの頻度で現在の損失関数の値を表示するか指定できます。デフォルトでは10000回に1回の頻度で表示させます。
　この値がイタレーションを重ねていくうちに小さくなっていけば、学習が上手く進んでいることになります。どの損失関数を使うかによって値の絶対値が異なってくるので、あくまで学習が進んでいるかの参考にしてください。
　

void SetRangeOfWeight(double min, double max)
　NNでは重みの初期値はランダムに決めますが、その下限と上限を指定します。なお、GAによる学習では、この範囲を超えないように重みが更新されていきます。


void SetPopulation(int population)
　遺伝的アルゴリズムにおける個体数を指定します。デフォルトでは100体となっています。


void SetMutationProbability(double prob)
　遺伝的アルゴリズムにおける、交叉の際に遺伝子が突然変異する確率を設定できます。デフォルトでは0.05になっています。


void SetNumOfDominantGene(double nDominantGene_)
　遺伝的アルゴリズムにおいて、適応度が高かった個体の数を指定します。交叉の際にはこれらの個体が親として選ばれます。


void PrintWeightMatrix()
　現在の重みを行列として表示させます。なお、bias　項も同時に表示します。


void PrintLastLayer(vec2D const &inputDataSet)
　全てのデータセットに対してその入力と出力の結果を表示させます。


void PrintLastLayer(vec1D const &inputData)
　ある一つの入力データに対して、その出力結果を表示させます。
　

vec1D::iterator GetOutputIterator(vec1D const &inputData)
　ある一つの入力データに対して、その出力結果のイテレータ(begin)を返します。なお、末尾の一つ先(end)のイテレータは、出力層のニューロンの数だけ先頭(begin)のイテレータに足したものになります。


vec1D::iterator GetLossIteratorBegin()
　損失関数の値は、SetLossPrintFrequencyで指定したイタレーション回数ごとに記録されます。このメンバ関数はその損失関数の値を記録した配列(vector<double>)の先頭(begin)イテレータを返します。


vec1D::iterator GetLossIteratorEnd()
　上で説明した損失関数の値を詰め込んだ配列の末尾+1を指す(end)イテレータを返します


void SaveNeuralNetwork(std::string output_filename)
　NNの重みおよび使用した活性化関数をテキストファイルに保存します。引数には拡張子なしで保存するファイル名を入力します。
　これにより、学習後のNNを保存し、次に説明するReadWeightMatrixで学習後の状態のNNを利用することが可能です。生成したファイル(パラメータファイル)は変更を加えないように気をつけてください。


void ReadWeightMatrix(std::string filename)
　メンバ関数 SaveNeuralNetwork で生成されたパラメータファイルを読み込んで、重み及び活性化関数を自動で設定します。なお、NNの構造はパラメータファイルを生成した時のNNの構造と一致させるようにしてください(NNの構造はパラメータファイルの一行目で確認することもできます)。




3. TestNN.cxx について
　TestNN.cxx にこのNNの使い方の例を記してます。まず、データを乱数を使って生成してます。入力のデータは 0 ~ 3 の実数を一様に発生させた物をそのまま使い、出力のデータ y には x に床関数を適応した値を入れています。
　なお、入力・出力ともに1次元ですが、サイズ1のvectorの配列としてデータを作ります。これらのデータの組を4000生成して NN に学習させます。NNの構造としては1:10:10:10:10:1としています。
　そのあと重みの値の範囲(-5 ~ 5)、活性化関数(Sigmoid, Identity)、学習法(Adam)を指定し、TrainNNでNNを学習させます。学習後、PrintWeightMatrixで学習した重みパラメータをファイル(test.txt)に保存しています。
　この例では書いていませんが、保存したパラメータファイルをReadWeightMatrix("ファイル名")として読み込むことで、学習したNNを使うことができるようになります。



4. 所感
　ニューラルネットワークそれ自体やその他畳み込みニューラルネットワーク(CNN)などは、すでに多くのライブラリやツールが出回っているのでそれを使えばもっと簡単にできると思います。
ただ、他の人が作ったものは仕様がいまいち分からなかったり、やりたいことを実現する方法が理解できなかったり、調べても広範的な知識が必要になる場面が多く、結局よく分からず痒いところに手が届かないことがしばしばありました。
そこで、自分の知っているものを組み合わせて一から作れば細かいところの調整もできるし、最初のステップとしては手っ取り早いと感じて(雑ですが)最終的にNNと同様なものが実装できました。おかげさまで内部の理論的部分の理解が進みました。
時間があればCNNの実装もやってみたいと思ってみたり。まだ、回帰の問題しか効力を確かめてないのでいずれ分類問題でも通用するか実験してみようと考えてます。
　色々と回帰問題をNNに解かせているうちに、各学習法や、活性化関数に対する非常に雑な感想を持ったので参考までに記します。
隠れ層の活性化関数は一般的にはReLUが良いとされていますが、僕が実装したNNでは、ReLUはむしろ学習が進まない印象でした。たまに損失関数の値がオーバーフローしちゃうのでその原因を探る必要がありそうです。
結局隠れ層の活性化関数はSigmoidが一番良い、というのが現在の印象です。出力層の活性化関数はどれを使ってもそれなりに良い学習結果になったので、一般的な活用の仕方さえすれば問題ないかなと思います。
学習法は逆誤差伝播法に基づいたものは現在は3種類しかないですが(SGD, Momentum SGD, Adam)、SGDはどのデータに対してもあまり学習が進まないようです。Momentum SGDとAdamについては回帰問題に対してはどちらも十分に学習が進みました。
回帰したい関数が滑らかな連続関数の場合はMomentum SGDがより優れていて、滑らかではない連続関数ないし非連続関数に対してはAdamが向いているという印象でした。
また、Adamを使用した場合はSGDとMomentum SGDに比べて少し学習が遅くなってしまうというデメリットがあります。とはいえ、学習に関しては多くの場合Adamが優れていると感じました。時間があれば他の学習法も実装できれば、と思います。

最後に：Githubでtxtファイルを保存する正式なやり方がわからなかった。
